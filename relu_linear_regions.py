# -*- coding: utf-8 -*-
"""relu-linear-regions.ipynb

Automatically generated by Colaboratory.

Copied from Kristoffers 2d information plane script

Original file is located at
    https://colab.research.google.com/drive/1p0YCE-mYRxX4i_hep7K4Sr7k_QRqDCDX
"""

#@title Load packages and data
import sys
import scipy
import random
import torch as th
import numpy as np
import torch.nn as nn
import seaborn as sns
import torch.nn.init as init
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import math

from sklearn import preprocessing
from sklearn.decomposition import PCA
from IPython.display import clear_output
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import KernelCenterer
from sklearn.metrics.pairwise import pairwise_kernels
from sklearn.datasets import make_moons, make_circles, make_blobs

import torch.utils.data as Data
# gpu = th.cuda.is_available()
# device = 'cuda' if gpu else 'cpu'
device = 'cpu'

N = 1000

# Create dataset
# x_tr, y_tr = make_blobs(N, centers=[[8, 8], [5, 5]])
# x_tr, y_tr = make_circles(N, noise=0.1, factor=0.25)

#@title Creating data -- Just a sinusoid in the initial case
train_data_length = 1024

x_tr = 2 * math.pi * th.rand(train_data_length)
y_tr = th.sin(x_tr)

x_tr = preprocessing.minmax_scale(x_tr)

plt.figure(1, figsize=(8, 6))
plt.scatter(x_tr, y_tr)
plt.show()

x_tr = th.tensor(x_tr, dtype=th.float, device=device)
x_tr = x_tr[:, None]
y_tr = th.tensor(y_tr, dtype=th.float, device=device)
# y_tr = nn.functional.one_hot(y_tr.squeeze(), num_classes=2).float()

#@title Network
import random
import torch.nn as nn
import torch.nn.init as init


# Simple neural network
dim, n_hidden_1, n_hidden_2, out_dim = 1, 200, 100, 1
class DNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.model = nn.Sequential(
                nn.Linear(dim, n_hidden_1),
                nn.ReLU(),
                nn.Linear(n_hidden_1, n_hidden_2),
                nn.ReLU(),
                nn.Linear(n_hidden_2, out_dim),
                )

        def forward(self, x):
            output = self.model(x)
            return output

dataset = Data.TensorDataset(x_tr, y_tr)

loader = Data.DataLoader(
    dataset=dataset,
    batch_size=32,
    shuffle=True
)
#@title train network
model = DNet()

epochs, L, acc = 1000, [], []
optimizer = th.optim.Adam(model.parameters(), lr=.01)

# Changed to MSELoss
# criterion = nn.CrossEntropyLoss()
criterion = nn.MSELoss()

# Use batches to train
for epoch in range(epochs):

    for step, (bx, by) in enumerate(loader):

        prediction = model(bx)

        loss = criterion(prediction, by)

        L.append([loss.item()])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print(epoch, L[-1])



plt.figure(1)
plt.subplot(1, 2, 1)
plt.plot(acc)
plt.title('accuracy')
plt.subplot(1, 2, 2)
plt.plot(L)
plt.title('Loss')

# Testing the output
plt.figure(2)
plt.plot(x_tr, model(x_tr).detach().numpy(), '.')
plt.plot(x_tr, y_tr, '.')
plt.show()

